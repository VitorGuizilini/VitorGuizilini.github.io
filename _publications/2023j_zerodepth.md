---
title: '
<a href="http://sites.google.com/view/tri-zerodepth" style="text-decoration:none;color:#000000;text-align:justify;"> 
Towards Zero-Shot Scale-Aware Monocular Depth Estimation
</a>
'
project: '
'
collection: publications
excerpt: '<p align=justify><a style="text-decoration:none; color:#000000; align:justify;">
Differentiable volumetric rendering is a powerful paradigm for 3D reconstruction and novel view synthesis. However, standard volume rendering approaches struggle with degenerate geometries in the case of limited viewpoint diversity, a common scenario in robotics applications. In this work, we propose to use the multi-view photometric objective from the self-supervised depth estimation literature as a geometric regularizer for volumetric rendering, significantly improving novel view synthesis without requiring additional information. Building upon this insight, we explore the explicit modeling of scene geometry using a generalist Transformer, jointly learning a radiance field as well as depth and light fields with a set of shared latent codes. We demonstrate that sharing geometric information across tasks is mutually beneficial, leading to improvements over single-task learning without an increase in network complexity. Our DeLiRa architecture achieves state-of-the-art results on the ScanNet benchmark, enabling high quality volumetric rendering as well as real-time novel view and depth synthesis in the limited viewpoint diversity setting.
</a></p>
<p><img src="/images/publications/zerodepth.png" align="right" width="100%" style="margin:0 0 20px 0"></p>
'
venue: '
<p>
Vitor Guizilini, Igor Vasiljevic, Dian Chen, Rares Ambrus, Adrien Gaidon
<br>
<b>ICCV 2023</b>
</p>
'
citation: '
@inproceedings{tri-zerodepth,
  title={Towards Zero-Shot Scale-Aware Monocular Depth Estimation},
  author={Guizilini, Vitor and Vasiljevic, Igor and Chen, Dian and Ambrus, Rares and Gaidon, Adrien},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month={October},
  year={2023},
}
'
links: '
<a href="https://arxiv.org/abs/2306.17253">
<img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png"></a>
<a href="https://github.com/TRI-ML/vidar">
<img style="margin-right:10px" align="right" width="40" src="/images/github.png"></a>
'
---
